{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "baca3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin  # Import the urljoin function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb30ab",
   "metadata": {},
   "source": [
    "=> Add \"&page=2...\" to the html_text url to go to the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8be46919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired content has been scraped and added to 'flipkart_reviews.csv' excel sheet.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "#For loop is used to iterate through first 10 pages by incrementing by 1 as the url ending increases by 1 as stated in the markdown line above.\n",
    "for num in range(1, 11):\n",
    "    html_text = requests.get(\"https://www.flipkart.com/apple-iphone-12-black-64-gb/product-reviews/itma2559422bf7c7?pid=MOBFWBYZU5FWK2VP&lid=LSTMOBFWBYZU5FWK2VPFMEI56&marketplace=FLIPKART\" + f\"&page={num}\").text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    reviews = soup.find_all('div', class_ = \"_27M-vq\")\n",
    "\n",
    "    for review in reviews:\n",
    "        customer_review = review.find('div', class_ = \"t-ZTKy\").text.replace('READ MORE', '')\n",
    "\n",
    "    #Created a table and appended the content into the table.\n",
    "        data.append({\"Customer Reviews\": customer_review})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    #Used to start the indexing from 1 as 0 is the default.\n",
    "    df.index = range(1, len(df) + 1)\n",
    "\n",
    "    styled_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "    styled_df.set_table_styles([{\n",
    "        'selector': 'th',\n",
    "        'props': [('text-align', 'left')]\n",
    "    }])\n",
    "\n",
    "    styled_df\n",
    "\n",
    "    #Saved the scraped content in an .csv format.\n",
    "    df.to_csv('flipkart_reviews.csv')\n",
    "    df.to_csv('flipkart_reviews.csv', index=True)\n",
    "    \n",
    "print(\"Desired content has been scraped and added to 'flipkart_reviews.csv' excel sheet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "222d74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mLink:\u001b[0m https://www.flipkart.com/apple-iphone-12-black-64-gb/product-reviews/itma2559422bf7c7?pid=MOBFWBYZU5FWK2VP&lid=LSTMOBFWBYZU5FWK2VPFMEI56&marketplace=FLIPKART&page=9\n"
     ]
    }
   ],
   "source": [
    "#Tried this at first but could not get far, as I could not figure out the logic.\n",
    "pages = soup.find_all('nav', class_ = \"yFHi8N\")\n",
    "for page in pages:\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    next_page = page.a['href']\n",
    "    full_link = urljoin(base_url, next_page)\n",
    "\n",
    "    print(f\"\\033[1m\\033[4mLink:\\033[0m {full_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148760f",
   "metadata": {},
   "source": [
    "=> TRIED THE METHODS BELOW BUT DIDN'T UNDERSTAND THE CONCEPT AS I TOOK IT FROM OUTSIDE SOURCES, HENCE WHY I WROTE MY OWN CODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a01128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for review in reviews:\n",
    "#    comment = review.find('div', class_ = False).text.replace('READ MORE', '')\n",
    "#    print(comment)\n",
    "\n",
    "\n",
    "#workbook = Workbook()\n",
    "#sheet = workbook.active\n",
    "\n",
    "#for index, element in enumerate(reviews, start = 1):\n",
    "#    data = element.get_text()\n",
    "#    sheet.cell(row=index, column=1, value=data)\n",
    "\n",
    "#excel_filename = 'flipkart_reviews.xlsx'\n",
    "#workbook.save(excel_filename)\n",
    "\n",
    "#print(f\"Scraped data saved to {excel_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
